{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "import json\n",
    "import requests\n",
    "\n",
    "query = \"fuel cell materials\".replace(\" \", '%20')\n",
    "# api_key = \"eb1c6904773529c49823f3fa8320d20b\"\n",
    "api_key = '78c1b783fb8039d53a5fc140809f974f' # me\n",
    "# api_key = 'e8beab4e5dbb6234709a2f9568f6f036'\n",
    "\n",
    "    \n",
    "def find_dois(year, start, max_return = 100):\n",
    "    global query, api_key\n",
    "\n",
    "    url = \"http://api.springernature.com/meta/v2/json?&q={}%20type:Journal%20year:{}&s={}&p={}&api_key={}\".format(query, year, start, max_return, api_key) \n",
    "    try:\n",
    "        crawl_content = request.urlopen(url).read()\n",
    "        Content = json.loads(crawl_content.decode('utf8'))\n",
    "        Dois = []\n",
    "        for i in range(max_return):\n",
    "            try:\n",
    "                Dois.append(Content['records'][i]['doi'])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # print(Dois)\n",
    "        return Dois\n",
    "    except Exception as e:\n",
    "        return []\n",
    "    \n",
    "def find_xml(papers):\n",
    "    global api_key\n",
    "\n",
    "    DoiUrls = []\n",
    "    for doi in papers:\n",
    "        doiUrls = \"https://api.springernature.com/meta/v2/pam?q=doi:{}&p=2&api_key={}\".format(doi,api_key)\n",
    "        DoiUrls.append(doiUrls)\n",
    "    return DoiUrls\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dois = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('year.txt') as f:\n",
    "    year = int(f.read().strip())\n",
    "\n",
    "if year > 1980:\n",
    "    result = []\n",
    "    print(year)\n",
    "    for i in range(51):\n",
    "        result += find_dois(year, i)\n",
    "        print(len(dois) + len(result), end = ' ')\n",
    "    print()\n",
    "    dois += result\n",
    "    with open('year.txt', 'w') as f:\n",
    "        f.write(str(year - 1))\n",
    "\n",
    "with open('springer_dois.json', 'a') as f:\n",
    "    for doi in dois:\n",
    "        f.write(doi)\n",
    "        f.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64535\n"
     ]
    }
   ],
   "source": [
    "with open('springer_dois.json') as f:\n",
    "    dois = list(set(f.read().strip().split('\\n')))\n",
    "    print(len(dois))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('springer_dois.json', 'w') as f:\n",
    "    for doi in dois:\n",
    "        f.write(doi)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60250 60300 60350 60400 60500 60550 60600 60650 60700 60750 60800 60850 60900 60950 61000 61050 61100 61150 61200 61250 61300 61350 61400 61450 61500 61550 61600 61650 61700 61750 61800 61850 61900 61950 62000 62050 62100 62150 62200 62250 62300 62350 62400 62450 62500 62550 62600 62650 62700 62750 62800 62850 62900 62950 63000 63050 63100 63150 63200 63250 63300 63350 63400 63450 63500 63550 63600 63650 63700 63750 63800 63850 63900 "
     ]
    }
   ],
   "source": [
    "xmls = find_xml(dois)\n",
    "with open('start.txt') as f: start = int(f.read().strip())\n",
    "\n",
    "for i in range(start, len(xmls)):\n",
    "    try:\n",
    "        content = request.urlopen(xmls[i]).read()\n",
    "        with open(r'springer_papers/springer_{}.xml'.format(i),'wb') as f:\n",
    "            if content is not None:2\n",
    "                f.write(content)\n",
    "                if i % 50 == 0: print(i, end = ' ')\n",
    "                with open('start.txt', 'w') as f: f.write(str(i))\n",
    "    except Exception:\n",
    "        content = None\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64535\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('springer_dois.json') as f:\n",
    "    dois = f.read().strip().split('\\n')\n",
    "    print(len(dois))\n",
    "\n",
    "# NOTE: 提取 DOI, abstract, title, 和 year\n",
    "def extract(folder, xml): \n",
    "    f = open(folder + '/' + xml, encoding = 'utf-8').read()\n",
    "    doc = BeautifulSoup(f, 'xml')\n",
    "    if doc.find('doi') is None: return\n",
    "    p_doi = doc.find('query').string[4:]\n",
    "    p_abst = None\n",
    "    p_title = None\n",
    "    p_year = None\n",
    "\n",
    "    if doc.find('title') != None and doc.find('title').string.lower() != 'abstract':\n",
    "        temp = doc.find('title').string.split()\n",
    "        p_title = ' '.join(temp)\n",
    "\n",
    "    if doc.find('publicationDate') != None:\n",
    "        p_year = int(doc.find('publicationDate').string[:4])\n",
    "\n",
    "    if doc.find('h1') != None and doc.find('h1').string.lower() == 'abstract': \n",
    "        temp = doc.find('h1').next_sibling.string.split()\n",
    "        p_abst = ' '.join(temp)\n",
    "    elif doc.find('p') != None and doc.find('p').string.lower() == 'abstract':\n",
    "        temp = doc.find('p').next_sibling.string.split()\n",
    "        p_abst = ' '.join(temp)\n",
    "    elif doc.find('title') != None and doc.find('title').string.lower() == 'abstract':\n",
    "        temp = doc.find('title').next_sibling.string.split()\n",
    "        p_abst = ' '.join(temp)\n",
    "\n",
    "    return (p_doi, p_abst, p_title, p_year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200 3400 3600 3800 4000 4200 4400 4600 4800 5000 5200 5400 5600 5800 6000 6200 6400 6600 6800 7000 7200 7400 7600 7800 8000 8200 8400 8600 8800 9000 9200 9400 9600 9800 10000 10200 10400 10600 10800 11000 11200 11400 11600 11800 12000 12200 12400 12600 12800 13000 13200 13400 13600 13800 14000 14200 14400 14600 14800 15000 15200 15400 15600 15800 16000 16200 16400 16600 16800 17000 17200 17400 17600 17800 18000 18200 18400 18600 18800 19000 19200 19400 19600 19800 20000 20200 20400 20600 20800 21000 21200 21400 21600 21800 22000 22200 22400 22600 22800 23000 23200 23400 23600 23800 24000 24200 24400 24600 24800 26400 26600 26800 27000 27200 27400 27600 27800 28000 28200 28400 28600 28800 29000 29200 29400 29600 29800 30000 30200 30400 30600 30800 31000 31200 31400 31600 31800 32000 32200 32400 32600 32800 33000 33200 33400 33600 33800 34000 34200 34400 34600 34800 35000 35200 35400 35600 35800 36000 36200 36400 36600 36800 37000 37200 37400 37600 37800 38000 38200 38400 38600 38800 39000 39200 39400 39600 39800 40000 40200 40400 40600 40800 41000 41200 41400 41600 41800 42000 42200 42400 42600 42800 43000 43200 43400 43600 43800 44000 44200 44400 44600 44800 45000 45200 45400 45600 45800 46000 46200 46400 46600 46800 47000 47200 47400 47600 47800 48000 48200 48400 48600 48800 49000 49200 49400 49600 49800 50000 50200 50400 50600 50800 51000 51200 51400 51600 51800 52000 52200 52400 52600 52800 53000 53200 53400 53600 53800 54000 54200 54400 54600 54800 55000 55200 55400 55600 55800 56000 56200 64534\n"
     ]
    }
   ],
   "source": [
    "# NOTE: 将论文装进字典里\n",
    "doi_absts = {}\n",
    "index = 0\n",
    "for index in range(0, len(dois)):\n",
    "    try:\n",
    "        name = f'springer_{index}.xml'\n",
    "        result = extract('springer_papers', name)\n",
    "        if result is None: continue\n",
    "        else: doi, abst, title, year = result\n",
    "        if year not in doi_absts: doi_absts[year] = [(doi, title, abst)]\n",
    "        else: doi_absts[year].append((doi, title, abst))\n",
    "        if index % 200 == 0: print(index, end = ' ')\n",
    "    except Exception: continue\n",
    "\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "2014 4241\n",
      "2003 448\n",
      "2015 4190\n",
      "2017 4198\n",
      "2021 4161\n",
      "2019 4326\n",
      "2011 2863\n",
      "2012 3524\n",
      "2009 1873\n",
      "2006 922\n",
      "2013 4303\n",
      "2020 4277\n",
      "2010 2141\n",
      "2018 4325\n",
      "2008 1742\n",
      "2005 617\n",
      "2016 4389\n",
      "2007 1257\n",
      "2002 166\n",
      "2004 534\n",
      "2001 192\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(len(doi_absts))\n",
    "for i in doi_absts: print(i, len(doi_absts[i]))\n",
    "# pprint(doi_absts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据库\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db_papers = client['papers']\n",
    "\n",
    "# 初始化数据库\n",
    "def papers_init(db_papers):\n",
    "    # for col in db_papers.list_collection_names():\n",
    "    #     de = db_papers[col]\n",
    "    #     db_papers.drop_collection(de)\n",
    "\n",
    "    for year in range(2001, 2022):\n",
    "        col = db_papers[str(year)]\n",
    "        for i in doi_absts[year]:\n",
    "            doi, title, abst = i\n",
    "            paper = {\n",
    "                'doi': doi,\n",
    "                'title': title,\n",
    "                'abst': abst\n",
    "            }\n",
    "            col.insert_one(paper)\n",
    "papers_init(db_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从数据库中读论文\n",
    "year_papers = {}\n",
    "# print(db_papers.list_collection_names())\n",
    "for year in db_papers.list_collection_names():\n",
    "    for paper in db_papers[year].find({}, {'_id': 0}):\n",
    "        if year not in year_papers: year_papers[year] = [(paper['doi'], paper['title'], paper['abst'])]\n",
    "        else: year_papers[year].append((paper['doi'], paper['title'], paper['abst']))\n",
    "# print(year_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17580778\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# 一共有多少单词\n",
    "text = []\n",
    "for year in year_papers:\n",
    "    for paper in year_papers[year]:\n",
    "        abst = paper[2]\n",
    "        \n",
    "        # print(doi_absts[doi])\n",
    "        if abst != None : text += abst.strip().split(' ')\n",
    "print(len(text))\n",
    "print(len(year_papers))\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "all_tokens = {}\n",
    "# freq = {} # 计算每个词出现的频率\n",
    "# for doi, (abst, title, year) in doi_absts.items():\n",
    "for year, paper_lst in year_papers.items():\n",
    "    # print(year_papers.items())\n",
    "    \n",
    "    index = 0\n",
    "    print()\n",
    "    print(year)\n",
    "    for paper in paper_lst:\n",
    "        doi, title, abst = paper\n",
    "    # for doi, title, abst in paper_lst:\n",
    "        if index % 500 == 0: print(index, end = ' ')\n",
    "        index += 1\n",
    "\n",
    "        doc = nlp(str(abst))\n",
    "        # print(doc)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and not token.is_stop:\n",
    "                if token.pos_ == 'NUM':\n",
    "                    tokens.append('#NUM')\n",
    "                    # if '#NUM' not in freq:\n",
    "                    #     freq['#NUM'] = 1\n",
    "                    # else: freq['#NUM'] += 1\n",
    "                else: \n",
    "                    tokens.append(token.lemma_)\n",
    "                    # if token.lemma_ not in freq: freq[token.lemma_] = 1\n",
    "                    # else: freq[token.lemma_] += 1\n",
    "\n",
    "        if year not in all_tokens: all_tokens[year] = [(doi, title, tokens)]\n",
    "        else: all_tokens[year].append((doi, title, tokens))\n",
    "    \n",
    "\n",
    "print(len(all_tokens))\n",
    "# TODO 去掉频率很低的词(目前论文数量太少，先不弄)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_tokens['2020'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 db_tokens\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db_tokens = client['s_tokens']\n",
    "def tokens_init(db_tokens):\n",
    "    for col in db_tokens.list_collection_names():\n",
    "        de = db_tokens[col]\n",
    "        db_tokens.drop_collection(de)\n",
    "\n",
    "    for col in db_tokens.list_collection_names():\n",
    "        de = db_tokens[col]\n",
    "        db_tokens.drop_collection(de)\n",
    "    for year in range(2000, 2022):\n",
    "        col = db_tokens[str(year)]\n",
    "        for i in all_tokens[str(year)]:\n",
    "            doi, title, tokens = i\n",
    "            paper = {\n",
    "                'doi': doi,\n",
    "                'title': title,\n",
    "                'tokens': tokens\n",
    "            }\n",
    "            col.insert_one(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从数据库中取 all_tokens\n",
    "all_tokens = {}\n",
    "for year in db_tokens.list_collection_names():\n",
    "    for paper in db_tokens[year].find({}, {'_id': 0}):\n",
    "        if year not in all_tokens: all_tokens[year] = [(paper['doi'], paper['title'], paper['tokens'])]\n",
    "        else: all_tokens[year].append((paper['doi'], paper['title'], paper['tokens']))\n",
    "\n",
    "# print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立语料库\n",
    "from gensim import utils\n",
    "\n",
    "class Corpus:\n",
    "    def __iter__(self):\n",
    "        for year in all_tokens:\n",
    "            token_lst = all_tokens[year]\n",
    "            for tup in token_lst:\n",
    "                tokens = tup[-1]\n",
    "                yield tokens\n",
    "\n",
    "corpus = Corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "all_texts = []\n",
    "for doi, tokens in all_tokens.items():\n",
    "    all_texts.append(tokens)\n",
    "# print(all_texts)\n",
    "dic = corpora.Dictionary(all_texts)\n",
    "# print(all_texts)\n",
    "print(dic.token2id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "1265565e84b9ee442e8495c811fced6702fec999448d037553e671d32787facd"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}