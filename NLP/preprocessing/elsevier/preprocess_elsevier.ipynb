{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web Scraping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE: 爬 DOI\n",
    "\n",
    "import json\n",
    "import requests\n",
    "# import pprint\n",
    "\n",
    "\n",
    "api_key = '43a3b963318fd18b917378a5f89e75c6'\n",
    "query = 'temperature'\n",
    "base_url = 'https://api.elsevier.com/content/search/sciencedirect'\n",
    "\n",
    "data = {\"qs\": query,\n",
    "        \"date\": 2019,\n",
    "        \"volume\": 1,  ### VOLUME 册\n",
    "        \"display\": {\n",
    "            \"show\": 100, # 展示多少篇文章\n",
    "            \"offset\": 1}} # 第几篇文章\n",
    "\n",
    "headers = {'x-els-apikey': '43a3b963318fd18b917378a5f89e75c6',\n",
    "           'Content-Type': 'application/json',\n",
    "           'Accept': 'application/json'}\n",
    "\n",
    "def get_response(url, data, headers):\n",
    "    response = requests.put(url, data=json.dumps(data), headers=headers) # 把本地要发出去的数据打包发到服务器\n",
    "    response = response.text.replace('false', 'False').replace('true', 'True')\n",
    "    try:\n",
    "        response = eval(response)\n",
    "    except:\n",
    "        print(response)\n",
    "    return response\n",
    "\n",
    "\n",
    "response = get_response(base_url, data, headers)\n",
    "dois=[]\n",
    "if 'results' in response.keys():\n",
    "    results = response['results']\n",
    "    for result in results:\n",
    "        if 'doi' in result:\n",
    "            dois.append(result['doi'])\n",
    "print(len(dois))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dois = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE: 爬论文\n",
    "import requests\n",
    "\n",
    "with open('elsevier_dois.json') as f:\n",
    "    dois = f.read().strip().split('\\n')\n",
    "    print(len(dois))\n",
    "\n",
    "api_key = '43a3b963318fd18b917378a5f89e75c6'\n",
    "with open('start.txt') as f: start = int(f.read().strip())\n",
    "\n",
    "for i in range(start, len(dois)):\n",
    "# for i in range(1802, 1803):\n",
    "    try:\n",
    "        with open(f'elsevier_papers/elsevier_{i}.xml', 'w', encoding='utf-8') as f:\n",
    "            request_url = 'https://api.elsevier.com/content/article/doi/{}?apiKey={}&httpAccept=text%2Fxml'.format(dois[i], api_key)\n",
    "            text = requests.get(request_url).text\n",
    "            if text is not None:\n",
    "                f.write(text)\n",
    "                if i % 50 == 0: print(i, end = ' ')\n",
    "                with open('start.txt', 'w') as f: f.write(str(i))\n",
    "    except Exception:\n",
    "        text = None\n",
    "        continue\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import requests\n",
    "\n",
    "with open('dois.json') as f:\n",
    "    dois = f.read().strip().split('\\n')\n",
    "    print(len(dois))\n",
    "\n",
    "api_key = '43a3b963318fd18b917378a5f89e75c6'\n",
    "with open('start.txt') as f: start = int(f.read().strip())\n",
    "\n",
    "for i in range(start, len(dois)):\n",
    "# for i in range(1802, 1803):\n",
    "    try:\n",
    "        with open(f'elsevier_papers/elsevier_{i}.xml', 'w', encoding='utf-8') as f:\n",
    "            request_url = 'https://api.elsevier.com/content/article/doi/{}?apiKey={}&httpAccept=text%2Fxml'.format(dois[i], api_key)\n",
    "            text = requests.get(request_url).text\n",
    "            if text is not None:\n",
    "                f.write(text)\n",
    "                if i % 50 == 0: print(i, end = ' ')\n",
    "                with open('start.txt', 'w') as f: f.write(str(i))\n",
    "    except Exception:\n",
    "        text = None\n",
    "        continue\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3273422\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('elsevier_dois.json') as f:\n",
    "    dois = f.read().strip().split('\\n')\n",
    "    print(len(dois))\n",
    "\n",
    "# NOTE: 提取 DOI, abstract, title, 和 year\n",
    "def extract(folder, xml): \n",
    "# def extract(paper):\n",
    "    f = open(folder + '/' + xml, encoding = 'utf-8').read()\n",
    "    doc = BeautifulSoup(f, 'xml')\n",
    "    # print(doc)\n",
    "    if doc.find('doi') is None: return\n",
    "    p_doi = doc.find('doi').string\n",
    "    p_abst = None\n",
    "    p_title = None\n",
    "    p_year = None\n",
    "    \n",
    "    if doc.find('title') != None and doc.find('title').string != None:\n",
    "        temp = doc.find('title').string.split()\n",
    "        p_title = ' '.join(temp)\n",
    "\n",
    "    if doc.find('coverDisplayDate') != None and doc.find('coverDisplayDate').string != None:\n",
    "        p_year = int(doc.find('coverDisplayDate').string.strip().split(' ')[-1][:4])\n",
    "        # print(p_year)\n",
    "    if doc.find('description') != None and doc.find('description').string != None:\n",
    "        # p_abst = doc.find('description').string.strip()\n",
    "        temp = doc.find('description').string.split()\n",
    "        p_abst = ' '.join(temp)\n",
    "        # print(p_abst)\n",
    "    \n",
    "    \n",
    "    return (p_doi, p_abst, p_title, p_year)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from LimeSoup import (\n",
    "    ACSSoup, \n",
    "    # AIPSoup,\n",
    "    APSSoup, \n",
    "    ECSSoup, \n",
    "    ElsevierSoup, \n",
    "    IOPSoup, \n",
    "    NatureSoup, \n",
    "    RSCSoup, \n",
    "    SpringerSoup, \n",
    "    WileySoup,\n",
    ")\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def findAllFiles(base):\n",
    "    result = []\n",
    "    for root, ds, fs in os.walk(base):\n",
    "        # print(fs)\n",
    "        result += fs\n",
    "    return result\n",
    "\n",
    "def find_content(dic):\n",
    "    result = ''\n",
    "    if isinstance(dic, str): \n",
    "        result += dic + ' '\n",
    "        return result\n",
    "    if isinstance(dic, list):\n",
    "        for l in dic:\n",
    "            result += find_content(l)\n",
    "    if isinstance(dic, dict):\n",
    "        result += find_content(dic['content'])\n",
    "    return result\n",
    "        \n",
    "\n",
    "\n",
    "def rsc_extract(html):\n",
    "    data = RSCSoup.parse(html)\n",
    "\n",
    "    doi, title, abstract = '', '', ''\n",
    "\n",
    "    year = BeautifulSoup(html, 'html').find('meta', {'name': 'DC.Date'}).get('content')[:4]\n",
    "\n",
    "    if 'DOI' in data: doi = data['DOI']\n",
    "    if 'Title' in data: title = data['Title']\n",
    "    if 'Sections' in data: \n",
    "        result = ''\n",
    "        for dic in data['Sections']:\n",
    "            result += find_content(dic)\n",
    "        abst = result\n",
    "\n",
    "    return doi, abst, title, year"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "doi_absts = {}\n",
    "index = 0\n",
    "for fname in findAllFiles('rsc1'):\n",
    "    with open(f'rsc1/{fname}', 'r', encoding = 'utf-8') as f:\n",
    "        html = f.read()\n",
    "    # print(fname)\n",
    "    try:\n",
    "        result = rsc_extract(html)\n",
    "        index += 1\n",
    "        temp = index\n",
    "        if result is None: \n",
    "            continue\n",
    "        else: doi, abst, title, year = result\n",
    "        if year not in doi_absts: doi_absts[year] = [(doi, title, abst)]\n",
    "        else: doi_absts[year].append((doi, title, abst))\n",
    "        if index % 50 == 0: print(index, end = ' ')\n",
    "    except Exception: continue\n",
    "\n",
    "print(index)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 1000 1050 1100 1150 1200 1250 1300 1350 1400 1450 1500 1550 1600 1650 1700 1750 1800 1850 1900 1950 2000 2050 2100 2150 2200 2250 2300 2350 2400 2450 2500 2550 2600 2650 2700 2750 2800 2850 2900 2950 3000 3050 3100 3150 3200 3250 3300 3350 3400 3450 3500 3550 3600 3650 3700 3750 3800 3850 3900 3950 4000 4050 4100 4150 4200 4250 4300 4350 4400 4404\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "years = []\n",
    "for year in doi_absts:\n",
    "    years.append(year)\n",
    "years.sort()\n",
    "print(years)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '1906', '1922', '1956', '1959', '1961', '1962', '1963', '1964', '1965', '1970', '1971', '1983', '1985', '1986', '1988', '1991', '1992', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE: 将论文装进字典里\n",
    "doi_absts = {}\n",
    "index = 0\n",
    "while index < len(dois):\n",
    "    name = f'elsevier_{index}.xml'\n",
    "    result = extract('elsevier_papers', name)\n",
    "    index += 1\n",
    "    temp = index\n",
    "    if result is None: \n",
    "        continue\n",
    "    else: doi, abst, title, year = result\n",
    "    if year not in doi_absts: doi_absts[year] = [(doi, title, abst)]\n",
    "    else: doi_absts[year].append((doi, title, abst))\n",
    "    if index % 300 == 0: print(index, end = ' ')\n",
    "\n",
    "print(index)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pprint import pprint\n",
    "print(len(doi_absts))\n",
    "# pprint(doi_absts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# NOTE: 创建数据库\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db_papers = client['papers']\n",
    "\n",
    "# NOTE: 初始化数据库\n",
    "def papers_init(db_papers):\n",
    "    # for col in db_papers.list_collection_names():\n",
    "    #     de = db_papers[col]\n",
    "    #     db_papers.drop_collection(de)\n",
    "\n",
    "    # for year in range(2000, 2022):\n",
    "    for year in years:\n",
    "        if year == '': year = 'None'\n",
    "        col = db_papers[str(year)]\n",
    "        if year == 'None': year = ''\n",
    "        for i in doi_absts[year]:\n",
    "            doi, title, abst = i\n",
    "            paper = {\n",
    "                'doi': doi,\n",
    "                'title': title,\n",
    "                'abst': abst\n",
    "            }\n",
    "            col.insert_one(paper)\n",
    "\n",
    "papers_init(db_papers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# NOTE: 从数据库中读论文\n",
    "year_papers = {}\n",
    "# print(db_papers.list_collection_names())\n",
    "for year in db_papers.list_collection_names():\n",
    "    for paper in db_papers[year].find({}, {'_id': 0}):\n",
    "        if year not in year_papers: year_papers[year] = [(paper['doi'], paper['title'], paper['abst'])]\n",
    "        else: year_papers[year].append((paper['doi'], paper['title'], paper['abst']))\n",
    "# print(year_papers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# NOTE: 一共有多少单词\n",
    "text = []\n",
    "for year in year_papers:\n",
    "    for paper in year_papers[year]:\n",
    "        abst = paper[2]\n",
    "        \n",
    "        # print(doi_absts[doi])\n",
    "        if abst != None : text += abst.strip().split(' ')\n",
    "print(len(text))\n",
    "print(len(year_papers))\n",
    "# print(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17580778\n",
      "22\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "text = []\n",
    "for year in doi_absts:\n",
    "    for paper in doi_absts[year]:\n",
    "        abst = paper[2]\n",
    "        \n",
    "        # print(doi_absts[doi])\n",
    "        if abst != None : text += abst.strip().split(' ')\n",
    "print(len(text))\n",
    "print(len(doi_absts))\n",
    "# print(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26101351\n",
      "46\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from chemdataextractor.doc import Document\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from mat2vec.processing import MaterialsTextProcessor\n",
    "\n",
    "processor = MaterialsTextProcessor()\n",
    "\n",
    "year_chemicals = {}\n",
    "\n",
    "for year, paper_lst in year_papers.items():\n",
    "    # index = 0\n",
    "    print(year)\n",
    "    result = ''\n",
    "    for tup in paper_lst:\n",
    "        \n",
    "        # if index % 100 == 0: print(index, end = ' ')\n",
    "        # index += 1\n",
    "        result += str(tup[1]) + \" \" + str(tup[2])\n",
    "    \n",
    "    print(len(result))\n",
    "    exclude = {'a', 'an', 'is', 'are', ',', \"=\", \".\", \",\", \"(\", \")\", \"<\", \">\", \"\\\"\", \"“\", \"”\", \"≥\", \"≤\", \"<nUm>\", \"been\", \"be\", \"are\",\n",
    "                 \"which\", \"were\", \"where\", \"have\", \"important\", \"has\", \"can\", \"or\", \"we\", \"our\",\n",
    "                 \"article\", \"paper\", \"show\", \"there\", \"if\", \"these\", \"could\", \"publication\",\n",
    "                 \"while\", \"measured\", \"measure\", \"demonstrate\", \"investigate\", \"investigated\",\n",
    "                 \"demonstrated\", \"when\", \"prepare\", \"prepared\", \"use\", \"used\", \"determine\",\n",
    "                 \"determined\", \"find\", \"successfully\", \"newly\", \"present\",\n",
    "                 \"reported\", \"report\", \"new\", \"characterize\", \"characterized\", \"experimental\",\n",
    "                 \"result\", \"results\", \"showed\", \"shown\", \"such\", \"after\",\n",
    "                 \"but\", \"this\", \"that\", \"via\", \"is\", \"was\", \"and\", \"using\"}\n",
    "    visited = set()\n",
    "    # result = ' '.join(set(result.split(' ')))\n",
    "    result = processor.process(str(result))[0]\n",
    "    temp = ''\n",
    "    for i in result:\n",
    "        if i in visited or i in exclude: continue\n",
    "        else:\n",
    "            visited.add(i)\n",
    "            temp += i + ' '\n",
    "    \n",
    "    print(len(temp))\n",
    "        \n",
    "    # print(temp)\n",
    "    # print('aaa')\n",
    "    \n",
    "    doc = Document(temp)\n",
    "    # print('aaa')\n",
    "    for dic in doc.records.serialize():\n",
    "        # print('a')\n",
    "        if 'names' in dic:\n",
    "            chemicals = set(dic['names'])\n",
    "            if year not in year_chemicals: year_chemicals[year] = chemicals\n",
    "            else: year_chemicals[year] = year_chemicals[year].union(chemicals)\n",
    "\n",
    "print(len(year_chemicals))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import regex\n",
    "import string\n",
    "import unidecode\n",
    "from os import path\n",
    "from monty.fractions import gcd_float\n",
    "\n",
    "from chemdataextractor.doc import Paragraph\n",
    "\n",
    "\n",
    "from pymatgen.core.periodic_table import Element\n",
    "from pymatgen.core.composition import Composition, CompositionError\n",
    "\n",
    "\n",
    "\n",
    "class MaterialsTextProcessor:\n",
    "    \"\"\"\n",
    "    Materials Science Text Processing Tools.\n",
    "    \"\"\"\n",
    "    ELEMENTS = [\"H\", \"He\", \"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"F\", \"Ne\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\", \"Cl\", \"Ar\", \"K\",\n",
    "                \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"As\", \"Se\", \"Br\", \"Kr\",\n",
    "                \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Sb\", \"Te\", \"I\",\n",
    "                \"Xe\", \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\", \"Yb\",\n",
    "                \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Hg\", \"Tl\", \"Pb\", \"Bi\", \"Po\", \"At\", \"Rn\", \"Fr\",\n",
    "                \"Ra\", \"Ac\", \"Th\", \"Pa\", \"U\", \"Np\", \"Pu\", \"Am\", \"Cm\", \"Bk\", \"Cf\", \"Es\", \"Fm\", \"Md\", \"No\", \"Lr\", \"Rf\",\n",
    "                \"Db\", \"Sg\", \"Bh\", \"Hs\", \"Mt\", \"Ds\", \"Rg\", \"Cn\", \"Nh\", \"Fl\", \"Mc\", \"Lv\", \"Ts\", \"Og\", \"Uue\"]\n",
    "\n",
    "    ELEMENT_NAMES = [\"hydrogen\", \"helium\", \"lithium\", \"beryllium\", \"boron\", \"carbon\", \"nitrogen\", \"oxygen\", \"fluorine\",\n",
    "                     \"neon\", \"sodium\", \"magnesium\", \"aluminium\", \"silicon\", \"phosphorus\", \"sulfur\", \"chlorine\", \"argon\",\n",
    "                     \"potassium\", \"calcium\", \"scandium\", \"titanium\", \"vanadium\", \"chromium\", \"manganese\", \"iron\",\n",
    "                     \"cobalt\", \"nickel\", \"copper\", \"zinc\", \"gallium\", \"germanium\", \"arsenic\", \"selenium\", \"bromine\",\n",
    "                     \"krypton\", \"rubidium\", \"strontium\", \"yttrium\", \"zirconium\", \"niobium\", \"molybdenum\", \"technetium\",\n",
    "                     \"ruthenium\", \"rhodium\", \"palladium\", \"silver\", \"cadmium\", \"indium\", \"tin\", \"antimony\", \"tellurium\",\n",
    "                     \"iodine\", \"xenon\", \"cesium\", \"barium\", \"lanthanum\", \"cerium\", \"praseodymium\", \"neodymium\",\n",
    "                     \"promethium\", \"samarium\", \"europium\", \"gadolinium\", \"terbium\", \"dysprosium\", \"holmium\", \"erbium\",\n",
    "                     \"thulium\", \"ytterbium\", \"lutetium\", \"hafnium\", \"tantalum\", \"tungsten\", \"rhenium\", \"osmium\",\n",
    "                     \"iridium\", \"platinum\", \"gold\", \"mercury\", \"thallium\", \"lead\", \"bismuth\", \"polonium\", \"astatine\",\n",
    "                     \"radon\", \"francium\", \"radium\", \"actinium\", \"thorium\", \"protactinium\", \"uranium\", \"neptunium\",\n",
    "                     \"plutonium\", \"americium\", \"curium\", \"berkelium\", \"californium\", \"einsteinium\", \"fermium\",\n",
    "                     \"mendelevium\", \"nobelium\", \"lawrencium\", \"rutherfordium\", \"dubnium\", \"seaborgium\", \"bohrium\",\n",
    "                     \"hassium\", \"meitnerium\", \"darmstadtium\", \"roentgenium\", \"copernicium\", \"nihonium\", \"flerovium\",\n",
    "                     \"moscovium\", \"livermorium\", \"tennessine\", \"oganesson\", \"ununennium\"]\n",
    "\n",
    "    ELEMENTS_AND_NAMES = ELEMENTS + ELEMENT_NAMES + [en.capitalize() for en in ELEMENT_NAMES]\n",
    "    ELEMENTS_NAMES_UL = ELEMENT_NAMES + [en.capitalize() for en in ELEMENT_NAMES]\n",
    "\n",
    "    # Elemement with the valence state in parenthesis.\n",
    "    ELEMENT_VALENCE_IN_PAR = regex.compile(r\"^(\"+r\"|\".join(ELEMENTS_AND_NAMES) +\n",
    "                                           r\")(\\(([IV|iv]|[Vv]?[Ii]{0,3})\\))$\")\n",
    "    ELEMENT_DIRECTION_IN_PAR = regex.compile(r\"^(\" + r\"|\".join(ELEMENTS_AND_NAMES) + r\")(\\(\\d\\d\\d\\d?\\))\")\n",
    "\n",
    "    # Exactly IV, VI or has 2 consecutive II, or roman in parenthesis: is not a simple formula.\n",
    "    VALENCE_INFO = regex.compile(r\"(II+|^IV$|^VI$|\\(IV\\)|\\(V?I{0,3}\\))\")\n",
    "\n",
    "    SPLIT_UNITS = {\"K\", \"h\", \"V\", \"wt\", \"wt.\", \"MHz\", \"kHz\", \"GHz\", \"Hz\", \"days\", \"weeks\",\n",
    "                   \"hours\", \"minutes\", \"seconds\", \"T\", \"MPa\", \"GPa\", \"kPa\", \"at.\", \"mol.\",\n",
    "                   \"at\", \"m\", \"N\", \"s-1\", \"vol.\", \"vol\", \"eV\", \"A\", \"atm\", \"bar\",\n",
    "                   \"kOe\", \"Oe\", \"h.\", \"mWcm−2\", \"keV\", \"MeV\", \"meV\", \"day\", \"week\", \"hour\",\n",
    "                   \"minute\", \"month\", \"months\", \"year\", \"cycles\", \"years\", \"fs\", \"ns\",\n",
    "                   \"ps\", \"rpm\", \"g\", \"mg\", \"mAcm−2\", \"mA\", \"mK\", \"mT\", \"s-1\", \"dB\",\n",
    "                   \"Ag-1\", \"mAg-1\", \"mAg−1\", \"mAg\", \"mAh\", \"mAhg−1\", \"m-2\", \"mJ\", \"kJ\",\n",
    "                   \"m2g−1\", \"THz\", \"KHz\", \"kJmol−1\", \"Torr\", \"gL-1\", \"Vcm−1\", \"mVs−1\",\n",
    "                   \"J\", \"GJ\", \"mTorr\", \"bar\", \"cm2\", \"mbar\", \"kbar\", \"mmol\", \"mol\", \"molL−1\",\n",
    "                   \"MΩ\", \"Ω\", \"kΩ\", \"mΩ\", \"mgL−1\", \"moldm−3\", \"m2\", \"m3\", \"cm-1\", \"cm\",\n",
    "                   \"Scm−1\", \"Acm−1\", \"eV−1cm−2\", \"cm-2\", \"sccm\", \"cm−2eV−1\", \"cm−3eV−1\",\n",
    "                   \"kA\", \"s−1\", \"emu\", \"L\", \"cmHz1\", \"gmol−1\", \"kVcm−1\", \"MPam1\",\n",
    "                   \"cm2V−1s−1\", \"Acm−2\", \"cm−2s−1\", \"MV\", \"ionscm−2\", \"Jcm−2\", \"ncm−2\",\n",
    "                   \"Jcm−2\", \"Wcm−2\", \"GWcm−2\", \"Acm−2K−2\", \"gcm−3\", \"cm3g−1\", \"mgl−1\",\n",
    "                   \"mgml−1\", \"mgcm−2\", \"mΩcm\", \"cm−2s−1\", \"cm−2\", \"ions\", \"moll−1\",\n",
    "                   \"nmol\", \"psi\", \"mol·L−1\", \"Jkg−1K−1\", \"km\", \"nm\", \"cm\", \"Wm−2\", \"mass\", \"mmHg\",\n",
    "                   \"mmmin−1\", \"GeV\", \"m−2\", \"m−2s−1\", \"Kmin−1\", \"gL−1\", \"ng\", \"hr\", \"w\", \"mW\", \"φ\", \"μ\", 'fv', \"RMF\", \n",
    "                   \"mN\", \"kN\", \"Mrad\", \"rad\", \"arcsec\", \"Ag−1\", \"dpa\", \"cdm−2\",\n",
    "                   \"cd\", \"mcd\", \"mHz\", \"m−3\", \"ppm\", \"phr\", \"mL\", \"ML\", \"mlmin−1\", \"MWm−2\",\n",
    "                   \"Wm−1K−1\", \"Wm−1K−1\", \"kWh\", \"Wkg−1\", \"Jm−3\", \"m-3\", \"gl−1\", \"A−1\",\n",
    "                   \"Ks−1\", \"mgdm−3\", \"mms−1\", \"ks\", \"appm\", \"ºC\", \"HV\", \"kDa\", \"Da\", \"kG\",\n",
    "                   \"kGy\", \"MGy\", \"Gy\", \"mGy\", \"Gbps\", \"μB\", \"μL\", \"μF\", \"nF\", \"pF\", \"mF\",\n",
    "                   \"A\", \"Å\", \"A˚\", \"μgL−1\"}\n",
    "    \n",
    "    STOPWORDS = []\n",
    "\n",
    "    NR_BASIC = regex.compile(r\"^[+-]?\\d*\\.?\\d+\\(?\\d*\\)?+$\", regex.DOTALL)\n",
    "    NR_AND_UNIT = regex.compile(r\"^([+-]?\\d*\\.?\\d+\\(?\\d*\\)?+)([\\p{script=Latin}|Ω|μ]+.*)\", regex.DOTALL)\n",
    "\n",
    "    PUNCT = list(string.punctuation) + [\"\\\"\", \"“\", \"”\", \"≥\", \"≤\", \"×\"]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.elem_name_dict = {en: es for en, es in zip(self.ELEMENT_NAMES, self.ELEMENTS)}\n",
    "        \n",
    "\n",
    "    def tokenize(self, text, split_oxidation=True, keep_sentences=True):\n",
    "        \"\"\"Converts a string to a list tokens (words) using a modified chemdataextractor tokenizer.\n",
    "\n",
    "        Adds a few fixes for inorganic materials science, such as splitting common units from numbers\n",
    "        and splitting the valence state.\n",
    "\n",
    "        Args:\n",
    "            text: input text as a string\n",
    "            split_oxidation: if True, will split the oxidation state from the element, e.g. iron(II)\n",
    "                will become iron (II), same with Fe(II), etc.\n",
    "            keep_sentences: if False, will disregard the sentence structure and return tokens as a\n",
    "                single list of strings. Otherwise returns a list of lists, each sentence separately.\n",
    "\n",
    "        Returns:\n",
    "            A list of strings if keep_sentence is False, otherwise a list of list of strings, which each\n",
    "            list corresponding to a single sentence.\n",
    "        \"\"\"\n",
    "        def split_token(token, so=split_oxidation):\n",
    "            \"\"\"Processes a single token, in case it needs to be split up.\n",
    "\n",
    "            There are 2 cases when the token is split: A number with a common unit, or an\n",
    "            element with a valence state.\n",
    "\n",
    "            Args:\n",
    "                token: The string to be processed.\n",
    "                so: If True, split the oxidation (valence) string. Units are always split.\n",
    "\n",
    "            Returns:\n",
    "                A list of strings.\n",
    "            \"\"\"\n",
    "            elem_with_valence = self.ELEMENT_VALENCE_IN_PAR.match(token) if so else None\n",
    "            nr_unit = self.NR_AND_UNIT.match(token)\n",
    "            if nr_unit is not None and nr_unit.group(2) in self.SPLIT_UNITS:\n",
    "                # Splitting the unit from number, e.g. \"5V\" -> [\"5\", \"V\"].\n",
    "                return [nr_unit.group(1), nr_unit.group(2)]\n",
    "                # return [nr_unit.group(1), '<UNIT>']\n",
    "            elif elem_with_valence is not None:\n",
    "                # Splitting element from it\"s valence state, e.g. \"Fe(II)\" -> [\"Fe\", \"(II)\"].\n",
    "                # return [elem_with_valence.group(1), elem_with_valence.group(2)]\n",
    "                return [elem_with_valence.group(1)]\n",
    "            else:\n",
    "                return [token]\n",
    "\n",
    "        cde_p = Paragraph(text)\n",
    "        tokens = cde_p.tokens\n",
    "        toks = []\n",
    "        for sentence in tokens:\n",
    "            if keep_sentences:\n",
    "                toks.append([])\n",
    "                for tok in sentence:\n",
    "                    toks[-1] += split_token(tok.text, so=split_oxidation)\n",
    "            else:\n",
    "                for tok in sentence:\n",
    "                    toks += split_token(tok.text, so=split_oxidation)\n",
    "        return toks\n",
    "\n",
    "    def process(self, tokens, exclude_punct=False, convert_num=True, normalize_materials=True, remove_accents=True,\n",
    "                split_oxidation=True):\n",
    "        \"\"\"Processes a pre-tokenized list of strings or a string.\n",
    "\n",
    "        Selective lower casing, material normalization, etc.\n",
    "\n",
    "        Args:\n",
    "            tokens: A list of strings or a string. If a string is supplied, will use the\n",
    "                tokenize method first to split it into a list of token strings.\n",
    "            exclude_punct: Bool flag to exclude all punctuation.\n",
    "            convert_num: Bool flag to convert numbers (selectively) to <nUm>.\n",
    "            normalize_materials: Bool flag to normalize all simple material formula.\n",
    "            remove_accents: Bool flag to remove accents, e.g. Néel -> Neel.\n",
    "            make_phrases: Bool flag to convert single tokens to common materials science phrases.\n",
    "            split_oxidation: Only used if string is supplied, see docstring for tokenize method.\n",
    "\n",
    "        Returns:\n",
    "            A (processed_tokens, material_list) tuple. processed_tokens is a list of strings,\n",
    "            whereas material_list is a list of (original_material_string, normalized_material_string)\n",
    "            tuples.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(tokens, list):  # If it\"s a string.\n",
    "            return self.process(self.tokenize(\n",
    "                tokens, split_oxidation=split_oxidation, keep_sentences=False),\n",
    "                exclude_punct=exclude_punct,\n",
    "                convert_num=convert_num,\n",
    "                normalize_materials=normalize_materials,\n",
    "                remove_accents=remove_accents,\n",
    "                \n",
    "            )\n",
    "\n",
    "        processed, mat_list = [], []\n",
    "\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if exclude_punct and tok in self.PUNCT:  # Punctuation.\n",
    "                continue\n",
    "            if len(tok) > 1 and (tok[-1] == '_' or tok[-1] == '-' or tok[-1] == '~' or tok[-1] == '^'): \n",
    "                # print('before:', tok)\n",
    "                tok = tok[:-1]\n",
    "                # print('after:', tok)\n",
    "            if len(tok) > 1 and (tok[0] == '_' or tok[0] == '-' or tok[0] == '~' or tok[0] == '^'):\n",
    "                # print('before:', tok)\n",
    "                tok = tok[1:]\n",
    "                # print('after:', tok)\n",
    "            elif tok in self.SPLIT_UNITS:\n",
    "                tok = '<UNIT>'\n",
    "            elif convert_num and self.is_number(tok):  # Number.\n",
    "                # Replace all numbers with <nUm>, except if it is a crystal direction (e.g. \"(111)\").\n",
    "                try:\n",
    "                    if tokens[i - 1] == \"(\" and tokens[i + 1] == \")\" \\\n",
    "                            or tokens[i - 1] == \"〈\" and tokens[i + 1] == \"〉\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        tok = \"<NUM>\"\n",
    "                except IndexError:\n",
    "                    tok = \"<NUM>\"\n",
    "            \n",
    "            elif tok in self.ELEMENTS_NAMES_UL:  # Chemical element name.\n",
    "                # Add as a material mention.\n",
    "                mat_list.append((tok, self.elem_name_dict[tok.lower()]))\n",
    "                tok = tok.lower()\n",
    "            elif self.is_simple_formula(tok):  # Simple chemical formula.\n",
    "                normalized_formula = self.normalized_formula(tok)\n",
    "                mat_list.append((tok, normalized_formula))\n",
    "                if normalize_materials:\n",
    "                    tok = normalized_formula\n",
    "            elif (len(tok) == 1 or (len(tok) > 1 and tok[0].isupper() and tok[1:].islower())) \\\n",
    "                    and tok not in self.ELEMENTS and tok not in self.SPLIT_UNITS \\\n",
    "                    and self.ELEMENT_DIRECTION_IN_PAR.match(tok) is None:\n",
    "                # To lowercase if only first letter is uppercase (chemical elements already covered above).\n",
    "                tok = tok.lower()\n",
    "\n",
    "            if remove_accents:\n",
    "                tok = self.remove_accent(tok)\n",
    "\n",
    "            processed.append(tok)\n",
    "\n",
    "        \n",
    "\n",
    "        return processed, mat_list\n",
    "\n",
    "    \n",
    "\n",
    "    def is_number(self, s):\n",
    "        \"\"\"Determines if the supplied string is number.\n",
    "\n",
    "        Args:\n",
    "            s: The input string.\n",
    "\n",
    "        Returns:\n",
    "            True if the supplied string is a number (both . and , are acceptable), False otherwise.\n",
    "        \"\"\"\n",
    "        return self.NR_BASIC.match(s.replace(\",\", \"\")) is not None\n",
    "\n",
    "    @staticmethod\n",
    "    def is_element(txt):\n",
    "        \"\"\"Checks if the string is a chemical symbol.\n",
    "\n",
    "        Args:\n",
    "            txt: The input string.\n",
    "\n",
    "        Returns:\n",
    "            True if the string is a chemical symbol, e.g. Hg, Fe, V, etc. False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            Element(txt)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def is_simple_formula(self, text):\n",
    "        \"\"\"Determines if the string is a simple chemical formula.\n",
    "\n",
    "        Excludes some roman numbers, e.g. IV.\n",
    "\n",
    "        Args:\n",
    "            text: The input string.\n",
    "\n",
    "        Returns:\n",
    "            True if the supplied string a simple formula, e.g. IrMn, LiFePO4, etc. More complex\n",
    "            formula such as LiFePxO4-x are not considered to be simple formulae.\n",
    "        \"\"\"\n",
    "        if self.VALENCE_INFO.search(text) is not None:\n",
    "            # 2 consecutive II, IV or VI should not be parsed as formula.\n",
    "            # Related to valence state, so don\"t want to mix with I and V elements.\n",
    "            return False\n",
    "        elif any(char.isdigit() or char.islower() for char in text):\n",
    "            # Aas to contain at least one lowercase letter or at least one number (to ignore abbreviations).\n",
    "            # Also ignores some materials like BN, but these are few and usually written in the same way,\n",
    "            # so normalization won\"t be crucial.\n",
    "            try:\n",
    "                if text in [\"O2\", \"N2\", \"Cl2\", \"F2\", \"H2\"]:\n",
    "                    # Including chemical elements that are diatomic at room temperature and atm pressure,\n",
    "                    # despite them having only a single element.\n",
    "                    return True\n",
    "                composition = Composition(text)\n",
    "                # Has to contain more than one element, single elements are handled differently.\n",
    "                if len(composition.keys()) < 2 or any([not self.is_element(key) for key in composition.keys()]):\n",
    "                    return False\n",
    "                return True\n",
    "            except (CompositionError, ValueError, OverflowError):\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def get_ordered_integer_formula(el_amt, max_denominator=1000):\n",
    "        \"\"\"Converts a mapping of {element: stoichiometric value} to a alphabetically ordered string.\n",
    "\n",
    "        Given a dictionary of {element : stoichiometric value, ..}, returns a string with\n",
    "        elements ordered alphabetically and stoichiometric values normalized to smallest common\n",
    "        integer denominator.\n",
    "\n",
    "        Args:\n",
    "            el_amt: {element: stoichiometric value} mapping.\n",
    "            max_denominator: The maximum common denominator of stoichiometric values to use for\n",
    "                normalization. Smaller stoichiometric fractions will be converted to the same\n",
    "                integer stoichiometry.\n",
    "\n",
    "        Returns:\n",
    "            A material formula string with elements ordered alphabetically and the stoichiometry\n",
    "            normalized to the smallest integer fractions.\n",
    "        \"\"\"\n",
    "        g = gcd_float(list(el_amt.values()), 1 / max_denominator)\n",
    "        d = {k: round(v / g) for k, v in el_amt.items()}\n",
    "        formula = \"\"\n",
    "        for k in sorted(d):\n",
    "            if d[k] > 1:\n",
    "                formula += k + str(d[k])\n",
    "            elif d[k] != 0:\n",
    "                formula += k\n",
    "        return formula\n",
    "\n",
    "    def normalized_formula(self, formula, max_denominator=1000):\n",
    "        \"\"\"Normalizes chemical formula to smallest common integer denominator, and orders elements alphabetically.\n",
    "\n",
    "        Args:\n",
    "            formula: the string formula.\n",
    "            max_denominator: highest precision for the denominator (1000 by default).\n",
    "\n",
    "        Returns:\n",
    "            A normalized formula string, e.g. Ni0.5Fe0.5 -> FeNi.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            formula_dict = Composition(formula).get_el_amt_dict()\n",
    "            return self.get_ordered_integer_formula(formula_dict, max_denominator)\n",
    "        except (CompositionError, ValueError):\n",
    "            return formula\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_accent(txt):\n",
    "        \"\"\"Removes accents from a string.\n",
    "\n",
    "        Args:\n",
    "            txt: The input string.\n",
    "\n",
    "        Returns:\n",
    "            The de-accented string.\n",
    "        \"\"\"\n",
    "        # There is a problem with angstrom sometimes, so ignoring length 1 strings.\n",
    "        return unidecode.unidecode(txt) if len(txt) > 1 else txt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "processor = MaterialsTextProcessor()\n",
    "\n",
    "year_tokens = {}\n",
    "\n",
    "for year, paper_lst in doi_absts.items():\n",
    "    index = 0\n",
    "    print(year)\n",
    "    for paper in paper_lst:\n",
    "        doi, title, abst = paper\n",
    "        if index % 50 == 0: print(index, end = ' ')\n",
    "        index += 1\n",
    "        # print(type(abst))\n",
    "        # break\n",
    "        tokens, chemicals = processor.process(str(abst))\n",
    "\n",
    "        if year not in year_tokens: year_tokens[year] = [(doi, title, tokens)]\n",
    "        else: year_tokens[year].append((doi, title, tokens))\n",
    "        \n",
    "    print()\n",
    "\n",
    "print(len(year_tokens))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2015\n",
      "0 50 100 150 200 250 300 350 400 450 \n",
      "2002\n",
      "0 \n",
      "2020\n",
      "0 50 100 150 200 250 300 350 400 450 \n",
      "2011\n",
      "0 50 100 150 \n",
      "2008\n",
      "0 50 \n",
      "2017\n",
      "0 50 100 150 200 250 300 350 400 \n",
      "2018\n",
      "0 50 100 150 200 250 300 350 \n",
      "2013\n",
      "0 50 100 150 200 250 300 \n",
      "2021\n",
      "0 50 100 150 \n",
      "2010\n",
      "0 50 100 \n",
      "2014\n",
      "0 50 100 150 200 250 300 350 400 \n",
      "2019\n",
      "0 50 100 150 200 250 300 350 \n",
      "\n",
      "0 \n",
      "2012\n",
      "0 50 100 150 200 250 \n",
      "2016\n",
      "0 50 100 150 200 250 300 350 400 450 \n",
      "2001\n",
      "0 \n",
      "2004\n",
      "0 \n",
      "2009\n",
      "0 50 \n",
      "1997\n",
      "0 \n",
      "2005\n",
      "0 \n",
      "2007\n",
      "0 \n",
      "2003\n",
      "0 \n",
      "1992\n",
      "0 \n",
      "2006\n",
      "0 \n",
      "1996\n",
      "0 \n",
      "1962\n",
      "0 \n",
      "1998\n",
      "0 \n",
      "1971\n",
      "0 \n",
      "1995\n",
      "0 \n",
      "1965\n",
      "0 \n",
      "1956\n",
      "0 \n",
      "1994\n",
      "0 \n",
      "1999\n",
      "0 \n",
      "1970\n",
      "0 \n",
      "1964\n",
      "0 \n",
      "2000\n",
      "0 \n",
      "1991\n",
      "0 \n",
      "1983\n",
      "0 \n",
      "1961\n",
      "0 \n",
      "1906\n",
      "0 \n",
      "1922\n",
      "0 \n",
      "1986\n",
      "0 \n",
      "1988\n",
      "0 \n",
      "1985\n",
      "0 \n",
      "1959\n",
      "0 \n",
      "1963\n",
      "0 \n",
      "46\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# from mat2vec.processing import MaterialsTextProcessor\n",
    "\n",
    "processor = MaterialsTextProcessor()\n",
    "\n",
    "year_tokens = {}\n",
    "\n",
    "for year, paper_lst in year_papers.items():\n",
    "    index = 0\n",
    "    print(year)\n",
    "    for paper in paper_lst:\n",
    "        doi, title, abst = paper\n",
    "        if index % 500 == 0: print(index, end = ' ')\n",
    "        index += 1\n",
    "        # print(type(abst))\n",
    "        # break\n",
    "        tokens, chemicals = processor.process(str(abst))\n",
    "\n",
    "        if year not in year_tokens: year_tokens[year] = [(doi, title, tokens)]\n",
    "        else: year_tokens[year].append((doi, title, tokens))\n",
    "        \n",
    "    print()\n",
    "\n",
    "print(len(year_tokens))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(year_tokens['2018'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE: 将 abstract 转为 tokens\n",
    "import spacy, re\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "all_tokens = {}\n",
    "# freq = {} # 计算每个词出现的频率\n",
    "# for doi, (abst, title, year) in doi_absts.items():\n",
    "for year, paper_lst in year_papers.items():\n",
    "    # print(year_papers.items())\n",
    "    \n",
    "    index = 0\n",
    "    print()\n",
    "    print(year)\n",
    "    for paper in paper_lst:\n",
    "        doi, title, abst = paper\n",
    "    # for doi, title, abst in paper_lst:\n",
    "        # print(index)\n",
    "        if index % 500 == 0: print(index, end = ' ')\n",
    "        index += 1\n",
    "\n",
    "        doc = nlp(str(abst))\n",
    "        # print(doc)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # print(token)\n",
    "            if re.match(r'(x|X)+(- | _)', token.shape_):\n",
    "                print(token, end = ' ')\n",
    "                token = nlp(str(token)[:-1])[0]\n",
    "                print(token)\n",
    "            if token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and not token.is_stop:\n",
    "                if token.pos_ == 'NUM':\n",
    "                    tokens.append('#NUM')\n",
    "                    # if '#NUM' not in freq:\n",
    "                    #     freq['#NUM'] = 1\n",
    "                    # else: freq['#NUM'] += 1\n",
    "                else: \n",
    "                    tokens.append(token.lemma_)\n",
    "                    # if token.lemma_ not in freq: freq[token.lemma_] = 1\n",
    "                    # else: freq[token.lemma_] += 1\n",
    "\n",
    "        if year not in all_tokens: all_tokens[year] = [(doi, title, tokens)]\n",
    "        else: all_tokens[year].append((doi, title, tokens))\n",
    "    \n",
    "\n",
    "print(len(all_tokens))\n",
    "# TODO 去掉频率很低的词(目前论文数量太少，先不弄)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# NOTE: 初始化 db_tokens\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db_tokens = client['year_tokens']\n",
    "def tokens_init(db_tokens):\n",
    "    # for col in db_tokens.list_collection_names():\n",
    "    #     de = db_tokens[col]\n",
    "    #     db_tokens.drop_collection(de)\n",
    "\n",
    "    # for year in range(2000, 2022):\n",
    "    for year in years:\n",
    "        if year == '': year = 'None'\n",
    "        col = db_tokens[str(year)]\n",
    "        if year == 'None': year = ''\n",
    "        for i in all_tokens[str(year)]:\n",
    "            doi, title, tokens = i\n",
    "            paper = {\n",
    "                'doi': doi,\n",
    "                'title': title,\n",
    "                'tokens': tokens\n",
    "            }\n",
    "            col.insert_one(paper)\n",
    "tokens_init(db_tokens)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'all_tokens' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d680bee20cec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m             }\n\u001b[1;32m     22\u001b[0m             \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtokens_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-d680bee20cec>\u001b[0m in \u001b[0;36mtokens_init\u001b[0;34m(db_tokens)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mdoi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             paper = {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_tokens' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# NOTE: 初始化 db_tokens_punct(带标点)\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db_tokens_punct = client['year_tokens_punct']\n",
    "def tokens_init(db_tokens_punct):\n",
    "    # for col in db_tokens_punct.list_collection_names():\n",
    "    #     de = db_tokens_punct[col]\n",
    "    #     db_tokens_punct.drop_collection(de)\n",
    "\n",
    "    # for year in range(2000, 2022):\n",
    "    for year in years:\n",
    "        if year == '': year = \"None\"\n",
    "        col = db_tokens_punct[str(year)]\n",
    "        if year == 'None': year = ''\n",
    "        for i in year_tokens[str(year)]:\n",
    "            doi, title, tokens = i\n",
    "            paper = {\n",
    "                'doi': doi,\n",
    "                'title': title,\n",
    "                'tokens': tokens\n",
    "            }\n",
    "            col.insert_one(paper)\n",
    "tokens_init(db_tokens_punct)\n",
    "print(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# NOTE: 初始化 db_tokens_punct_test(带标点)\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db_tokens_punct = client['year_tokens_punct_test']\n",
    "def tokens_init(db_tokens_punct):\n",
    "    for col in db_tokens_punct.list_collection_names():\n",
    "        de = db_tokens_punct[col]\n",
    "        db_tokens_punct.drop_collection(de)\n",
    "\n",
    "    for year in range(2000, 2022):\n",
    "        col = db_tokens_punct[str(year)]\n",
    "        for i in year_tokens[str(year)]:\n",
    "            doi, title, tokens = i\n",
    "            paper = {\n",
    "                'doi': doi,\n",
    "                'title': title,\n",
    "                'tokens': tokens\n",
    "            }\n",
    "            col.insert_one(paper)\n",
    "tokens_init(db_tokens_punct)\n",
    "print(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(year_chemicals)\n",
    "for year in range(2000, 2022):\n",
    "    result = set()\n",
    "    for tup in year_chemicals[str(year)]:\n",
    "        for i in tup:\n",
    "            result.add(i)\n",
    "    year_chemicals[str(year)] = result\n",
    "print(year_chemicals)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# NOTE: 初始化 db_chemicals\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db_chemicals = client['year_chemicals']\n",
    "def chemicals_init(db_chemicals):\n",
    "    for col in db_chemicals.list_collection_names():\n",
    "        de = db_chemicals[col]\n",
    "        db_chemicals.drop_collection(de)\n",
    "\n",
    "    for year in range(2000, 2022):\n",
    "        col = db_chemicals[str(year)]\n",
    "        \n",
    "        item = {\n",
    "            'chemical': list(year_chemicals[str(year)])\n",
    "        }\n",
    "        col.insert_one(item)\n",
    "\n",
    "chemicals_init(db_chemicals)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# NOTE: 从数据库中取 all_tokens\n",
    "all_tokens = {}\n",
    "for year in db_tokens.list_collection_names():\n",
    "    for paper in db_tokens[year].find({}, {'_id': 0}):\n",
    "        if year not in all_tokens: all_tokens[year] = [(paper['doi'], paper['title'], paper['tokens'])]\n",
    "        else: all_tokens[year].append((paper['doi'], paper['title'], paper['tokens']))\n",
    "\n",
    "# print(all_tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# NOTE: 从数据库中取 all_tokens_punct\n",
    "all_tokens = {}\n",
    "for year in db_tokens_punct.list_collection_names():\n",
    "    for paper in db_tokens_punct[year].find({}, {'_id': 0}):\n",
    "        if year not in all_tokens: all_tokens[year] = [(paper['doi'], paper['title'], paper['tokens'])]\n",
    "        else: all_tokens[year].append((paper['doi'], paper['title'], paper['tokens']))\n",
    "\n",
    "# print(all_tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(all_tokens_punct)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# NOTE: 建立语料库\n",
    "from gensim import utils\n",
    "\n",
    "class Corpus:\n",
    "    def __iter__(self):\n",
    "        for year in all_tokens:\n",
    "            token_lst = all_tokens[year]\n",
    "            for tup in token_lst:\n",
    "                tokens = tup[-1]\n",
    "                yield tokens\n",
    "\n",
    "corpus = Corpus()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE: Word2Vec\n",
    "import gensim.models\n",
    "\n",
    "# for i in corpus: print(i)\n",
    "w2v_model = gensim.models.Word2Vec(corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE: FastText\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "f_model = FastText()\n",
    "f_model.build_vocab(corpus)\n",
    "\n",
    "f_model.train(corpus_iterable=corpus, epochs=f_model.epochs, total_examples=f_model.corpus_count, total_words=f_model.corpus_total_words)\n",
    "print(f_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NOTE: To store models\n",
    "import tempfile\n",
    "def store_w2v():\n",
    "    with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "        w2v_path = tmp.name\n",
    "        with open('w2v_model_path', 'w') as f:\n",
    "            f.write(w2v_path)\n",
    "        w2v_model.save(w2v_path)\n",
    "\n",
    "def store_f():\n",
    "    with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "        f_path = tmp.name\n",
    "        with open('f_model_path', 'w') as f:\n",
    "            f.write(f_path)\n",
    "        f_model.save(f_path)\n",
    "\n",
    "# store_w2v()\n",
    "# store_f()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# NOTE: To load models\n",
    "def load_w2v():\n",
    "    with open('w2v_model_path') as f:\n",
    "        w2v_path = f.read()\n",
    "    return gensim.models.Word2Vec.load(w2v_path)\n",
    "\n",
    "def load_f():\n",
    "    with open('f_model_path') as f:\n",
    "        f_path = f.read()\n",
    "    return gensim.models.FastText.load(f_path)\n",
    "\n",
    "w2v_model = load_w2v()\n",
    "# f_model = load_f()\n",
    "# print(w2v_path, f_path)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import gensim'); }\n    "
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "w2v_model.wv.most_similar('good')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('excellent', 0.8196513056755066),\n",
       " ('well', 0.8011565804481506),\n",
       " ('reasonable', 0.7711325287818909),\n",
       " ('superior', 0.7699888944625854),\n",
       " ('satisfactory', 0.763913094997406),\n",
       " ('improved', 0.7163109183311462),\n",
       " ('outstanding', 0.6705237030982971),\n",
       " ('acceptable', 0.65166836977005),\n",
       " ('ideal', 0.5965924859046936),\n",
       " ('comparable', 0.5880300998687744)]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f_model.wv.most_similar(('good'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w2v_model.wv.doesnt_match(['manganese', 'zinc', 'good', 'carbon'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w2v_model.wv.most_similar(positive=['bad', 'excellent'], negative=['good'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    try:\n",
    "        result = w2v_model.wv.most_similar(positive=[y1, x2], negative=[x1])\n",
    "        return result[0][0]\n",
    "    except KeyError:\n",
    "        return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "analogy('good', 'excellent', 'bad')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'inferior'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_pca_scatterplot(w2v_model.wv, ['China', 'England', 'long', 'short', 'increase', 'metal', 'America', 'iron', 'carbon', 'fire', 'Mg', 'zinc', 'magnetic', 'decrease', 'nickel', 'good', 'excellent', 'bad', 'manganese'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dic = []\n",
    "all_texts = []\n",
    "for year in all_tokens:\n",
    "    token_lst = all_tokens[year]\n",
    "    for doi, title, tokens in token_lst:\n",
    "        all_texts += tokens\n",
    "        dic.append(tokens)\n",
    "    \n",
    "# print(all_texts)\n",
    "dic = corpora.Dictionary(dic)\n",
    "print(len(all_texts))\n",
    "print(len(dic))\n",
    "# print(dic.token2id)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 化学品名称数量\n",
    "from chemdataextractor.doc import Document\n",
    "\n",
    "doc = ''\n",
    "# print(all_texts)\n",
    "for text in all_texts[:20000]:\n",
    "    # print(text)\n",
    "    doc += text + ' '\n",
    "doc = Document(doc)\n",
    "print(len(doc.records.serialize()))\n",
    "# print(doc.records.serialize())\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "with open('analogies.txt') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "\n",
    "pos = 0\n",
    "neg = 0\n",
    "for line in lines:\n",
    "    if line[0] == ':': continue\n",
    "    words = line.strip().split(' ')\n",
    "    result = analogy(words[0], words[1], words[2])\n",
    "    if result is not None and result == words[3]: pos += 1\n",
    "    else: neg += 1\n",
    "\n",
    "print(pos, neg)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1705 33399\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1265565e84b9ee442e8495c811fced6702fec999448d037553e671d32787facd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "1265565e84b9ee442e8495c811fced6702fec999448d037553e671d32787facd"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}